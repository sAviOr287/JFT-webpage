<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Jean-François Ton — Academic Profile</title>
  <meta name="description" content="Academic profile of Jean-François Ton, Senior Research Scientist at ByteDance focusing on responsible large language model alignment." />
  <link rel="shortcut icon" href="Images/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" href="css/site.css" />
</head>
<body>
  <header id="top" class="site-header">
    <p class="site-header__title">Jean-François Ton</p>
    <nav class="site-nav" aria-label="Primary">
      <a href="#about">About</a>
      <a href="#research">Research Interests</a>
      <a href="#publications">Recent Publications</a>
      <a href="#contact">Contact</a>
    </nav>
  </header>

  <main>
    <section id="about" class="intro">
      <div class="intro__profile reveal-on-scroll">
        <figure class="intro__photo-wrap">
          <img class="intro__photo" src="Images/me_photo2.jpg" alt="Portrait of Jean-François Ton" loading="lazy" />
        </figure>
        <div class="intro__summary">
          <h1 class="intro__title">Jean-François Ton</h1>
          <p class="intro__role">Senior Research Scientist · ByteDance Research / SEED</p>
          <p class="intro__bio">I build alignment and evaluation tooling for large language models, with a focus on multi-agent collaboration, preference data pipelines, and uncertainty-aware reward modelling. My statistical training in kernel methods and causal inference shapes how I guide responsible deployment of foundation models.</p>
          <ul class="intro__topics">
            <li>LLM Alignment</li>
            <li>Multi-Agent Systems</li>
            <li>Kernel Methods</li>
            <li>Responsible AI</li>
          </ul>
          <p class="intro__location">London, United Kingdom</p>
        </div>
      </div>

      <div class="info-grid intro__details">
        <div>
          <h2 class="intro__subheading">Current focus</h2>
          <ul class="stack">
            <li>Responsible deployment of post-trained LLMs for high-stakes user interactions.</li>
            <li>Scalable data collection and uncertainty-aware reward modelling.</li>
            <li>Designing evaluations for cooperative multi-agent systems.</li>
          </ul>
        </div>
        <div>
          <h2 class="intro__subheading">Previous appointments</h2>
          <ul class="stack">
            <li>Research Scientist Intern, Amazon (Tübingen, 2021–2022).</li>
            <li>Research Scientist Intern, Apple (Cupertino, 2020–2021).</li>
            <li>Research roles at Bloomberg LP and Imperial College on statistical methods and reinforcement learning.</li>
          </ul>
        </div>
      </div>
      <div>
        <h2 class="intro__subheading">Education</h2>
        <ul class="education-list">
          <li class="education-item">
            <strong>DPhil, Statistics &amp; Machine Learning — University of Oxford</strong>
            <span>St Peter’s College · Sep 2017 – Sep 2022</span>
            <p class="note">Thesis: <em>Causal Reasoning and Meta Learning using Kernel Mean Embeddings</em>.</p>
          </li>
          <li class="education-item">
            <strong>Masters, Applied Statistics — University of Oxford</strong>
            <span>Somerville College · Oct 2016 – Aug 2017 · Distinction (Top 10%)</span>
          </li>
          <li class="education-item">
            <strong>BSc, Mathematics — Imperial College London</strong>
            <span>Oct 2013 – Aug 2016 · First Class Honours</span>
          </li>
        </ul>
      </div>
    </section>

    <section id="research">
      <h2>Research Interests</h2>
      <ul class="tag-list">
        <li class="tag">LLM Alignment &amp; Safety</li>
        <li class="tag">Preference &amp; Reward Modelling</li>
        <li class="tag">Multi-Agent Evaluation</li>
        <li class="tag">Causal Inference &amp; Uncertainty</li>
        <li class="tag">Efficient Learning Systems</li>
      </ul>
      <p class="smaller">I design data-efficient alignment methods, translate theoretical guarantees into practical evaluation tooling, and study how uncertainty estimates mitigate reward hacking in complex environments. Earlier work on kernel methods and causal discovery still shapes how I analyse LLM behaviour today.</p>
    </section>

    <section id="publications">
      <h2>Recent Publications</h2>
      <p class="smaller">Selected publications that illustrate ongoing themes in preference-based alignment, trustworthy deployment, and efficient learning.</p>
      <ul class="publication-list">
        <li class="publication">
          <strong>Active Reward Modeling: Adaptive Preference Labeling for Large Language Model Alignment</strong>
          <span>ICML 2025 — Yunyi Shen*, Hao Sun*, Jean-François Ton</span>
        </li>
        <li class="publication">
          <strong>Understanding Chain-of-Thought in LLMs through Information Theory</strong>
          <span>ICML 2025 — Jean-François Ton*, Muhammad Faaiz Taufiq, Yang Liu</span>
        </li>
        <li class="publication">
          <strong>ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate</strong>
          <span>ICLR 2025 — Jean-François Ton*, Andrew Estornell*, Yuanshun Yao, Yang Liu</span>
        </li>
        <li class="publication">
          <strong>Mitigating Reward Overoptimization via Lightweight Uncertainty</strong>
          <span>NeurIPS 2024 — Jean-François Ton*, Xiaoying Zhang*, Wei Shen, Hongning Wang, Yang Liu</span>
        </li>
        <li class="publication">
          <strong>Conformal Off-Policy Prediction in Contextual Bandits</strong>
          <span>NeurIPS 2022 — Jean-François Ton*, Muhammad Faaiz Taufiq*, Rob Cornish, Yee Whye Teh, Arnaud Doucet</span>
        </li>
      </ul>
      <p class="note">For a full list organised by topic, see the <a href="works.html">publications page</a>.</p>
    </section>

    <section id="contact">
      <h2>Contact</h2>
      <p>If you would like to discuss collaborations, speaking invitations, or share feedback on alignment research, please reach out using the links below.</p>
      <div class="link-row">
        <a href="mailto:jeanfrancois287@hotmail.fr">jeanfrancois287@hotmail.fr</a>
        <a href="https://www.linkedin.com/in/jean-fran%C3%A7ois-ton-b172a5102/" target="_blank" rel="noopener">LinkedIn</a>
        <a href="https://scholar.google.com/citations?user=WWVOu4kAAAAJ" target="_blank" rel="noopener">Google Scholar</a>
      </div>
    </section>
  </main>

  <footer class="footer">
    <div class="footer__inner">
      <span>© <span id="year"></span> Jean-François Ton</span>
      <ul class="list-inline" aria-label="Footer links">
        <li><a href="#about">About</a></li>
        <li><a href="#research">Research</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>
    </div>
  </footer>

  <script src="js/reveal.js" defer></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
</body>
</html>
